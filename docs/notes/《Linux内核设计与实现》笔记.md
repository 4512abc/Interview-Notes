《Linux内核设计与实现》笔记 ✏️⭐️

> **[参考：《Linux内核设计与实现》读书笔记](https://www.cnblogs.com/wang_yb/p/3514730.html)**



# 第一章 Linux内核简介

## 1. 单内核和微内核

|            | **原理**                                                     | **优势**                                                     | **劣势**                                                   |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------------------------------- |
| **单内核** | 整体上作为一个单独的大过程来实现，整个内核都在一个大内核地址空间上运行。 | 1. 简单。 2. 高效：所有内核都在一个大的地址空间上，所以内核各个功能之间的调用和调用函数类似，几乎没有性能开销。 | 一个功能的崩溃会导致整个内核无法使用。                     |
| **微内核** | 内核按功能被划分成多个独立的过程。每个过程独立的运行在自己的地址空间上。 | 1. 安全：内核的各种服务独立运行，一种服务挂了不会影响其他服务。 | 内核各个服务之间通过进程间通信互通消息，比较复杂且效率低。 |

因为 IPC 机制的开销多于函数调用，又因为会涉及内核空间与用户空间的上下文切换，因此，消息传递需要一定的周期，而单内核中简单的函数调用没有这些开销。

Linux的内核虽然是基于单内核的，运行在单独的内核地址空间上。但是经过这么多年的发展，也具备微内核的一些特征。（体现了Linux实用至上的原则）

主要有以下特征：

1. 模块化设计，支持动态加载内核模块
2. 支持对称多处理（SMP）
3. 内核可以抢占（preemptive），允许内核运行的任务有优先执行的能力
4. 支持内核线程，不区分线程和进程



## 2. 内核版本号

内核的版本号主要有四个数组组成。比如版本号：2.6.26.1 其中：

2 - **主版本号**

6 - **从版本号或副版本号**

26 - 修订版本号

1 - 稳定版本号

副版本号表示这个版本是稳定版（**偶数**）还是开发版（**奇数**），上面例子中的版本号是稳定版。

稳定的版本可用于企业级环境。

修订版本号的升级包括BUG修正，新的驱动以及新的特性的追加。

稳定版本号主要是一些关键性BUG的修改。



# 第二章 从内核出发

## 1. 获取内核源码

内核是开源的，所有获取源码特别方便，参照以下的网址，可以通过git或者直接下载压缩好的源码包。

[http://www.kernel.org](http://www.kernel.org/)



## 2. 内核源码的结构

| **目录**      | **说明**                            |
| ------------- | ----------------------------------- |
| arch          | 特定体系结构的代码                  |
| block         | 块设备I/O层                         |
| crypo         | 加密API                             |
| Documentation | 内核源码文档                        |
| drivers       | 设备驱动程序                        |
| firmware      | 使用某些驱动程序而需要的设备固件    |
| fs            | VFS和各种文件系统                   |
| include       | 内核头文件                          |
| init          | 内核引导和初始化                    |
| ipc           | 进程间通信代码                      |
| kernel        | 像调度程序这样的核心子系统          |
| lib           | 同样内核函数                        |
| mm            | 内存管理子系统和VM                  |
| net           | 网络子系统                          |
| samples       | 示例，示范代码                      |
| scripts       | 编译内核所用的脚本                  |
| security      | Linux 安全模块                      |
| sound         | 语音子系统                          |
| usr           | 早期用户空间代码（所谓的initramfs） |
| tools         | 在Linux开发中有用的工具             |
| virt          | 虚拟化基础结构                      |



## 3. 编译内核的方法

还未实际尝试过手动编译内核，只是用yum更新过内核。这部分等以后手动编译过再补上。

安装新的内核后，重启时会提示进入哪个内核。当多次安装新的内核后，启动列表会很长（因为有很多版本的内核），显得不是很方便。



下面介绍3种删除那些不用的内核的方法：(是如何安装的就选择相应的删除方法)

- rpm 删除法

    rpm -qa | grep kernel* (查找所有linux内核版本)
    rpm -e kernel-(想要删除的版本)

- yum 删除法

    yum remove kernel-(要删除的版本)

- 手动删除

    删除/lib/modules/目录下不需要的内核库文件
    删除/usr/src/kernel/目录下不需要的内核源码
    删除/boot目录下启动的核心档案和内核映像
    更改grub的配置，删除不需要的内核启动列表

 

## 4. 内核开发的特点

### 4.1 无标准C库

为了保证内核的小和高效，内核开发中不能使用C标准库，所以连最常用的printf函数也没有，但是还好有个printk函数来代替。



### 4.2 使用GNU C

因为使用GNU C，所有内核中常使用GNU C中的一些扩展：

- **内联函数**

  内联函数在编译时会在它被调用的地方展开，减少了函数调用的开销，性能较好。但是，频繁的使用内联函数也会使代码变长，从而在运行时占用更多的内存。

  所以内联函数使用时最好要满足以下几点：函数较小，会被反复调用，对程序的时间要求比较严格。

  内联函数示例：static **inline** void sample();

- **内联汇编**

  内联汇编用于偏近底层或对执行时间严格要求的地方。示例如下：

    ```C
    unsigned int low, high;
    asm volatile("rdtsc" : "=a" (low), "=d" (high));
    /* low 和 high 分别包含64位时间戳的低32位和高32位 */
    ```

- **分支声明**

  如果能事先判断一个if语句时经常为真还是经常为假，那么可以用unlikely和likely来优化这段判断的代码。

    ```C
    /* 如果error在绝大多数情况下为0(假) */
    if (unlikely(error)) {
        /* ... */
    }

    /* 如果success在绝大多数情况下不为0(真) */
    if (likely(success)) {
        /* ... */
    }
    ```



### 4.3 没有内存保护

因为内核是最低层的程序，所以如果内核访问的非法内存，那么整个系统都会挂掉！所以内核开发的风险比用户程序开发的风险要大。

内核中的内存是不分页的，每用一个字节的内存，物理内存就少一个字节。所以内核中使用内存一定要谨慎。



### 4.4 不使用浮点数

内核不能完美的支持浮点操作，使用浮点数时，需要人工保存和恢复浮点寄存器及其他一些繁琐的操作。



### 4.5 内核栈容积小且固定

内核栈的大小有编译内核时决定的，对于不用的体系结构，内核栈的大小虽然不一样，但都是固定的。

查看内核栈大小的方法：

```C
ulimit -a | grep "stack size"
```



### 4.6 同步和并发

Linux是多用户的操作系统，所以必须处理好同步和并发操作，防止因竞争而出现死锁。

内核很容易产生竞争条件。和单线程的用户空间程序不同，内核的许多特性都要求能够并发地访问共享数据，这就要求有同步机制以保证不出现竞争条件，特别是：

- **Linux 是抢占多任务操作系统**。内核的进程调度程序即兴对进程进行调度和重新调度。内核必须和这些任务同步。
- **Linux 内核支持对称多处理器系统 (SMP)** 。所以，如果没有适当的保护，同时在两个或两个以上的处理器上执行的内核代码很可能会同时访问共享的同一个资源。
- **中断是异步到来的**，完全不顾及当前正在执行的代码。也就是说，如果不加以适当的保护，中断完全有可能在代码访问资源的时候到来，这样，中段处理程序就有可能访问同一资源。
- **Linux 内核可以抢占**。所以，如果不加以适当的保护，内核中一段正在执行的代码可能会被另外一段代码抢占，从而有可能导致儿段代码同时访问相同的资源。

常用的解决竞争的办法是**自旋锁和信号量**。



### 4.7 可移植性

Linux内核可用于不用的体现结构，支持多种硬件。所以开发时要时刻注意可移植性，尽量使用体系结构无关的代码。



# 第三章 进程管理

## 1. 进程

**程序本身并不是进程，进程是处于执行期的程序以及相关的资源的总称。**

- 可能存在两个或多个不同的进程执行的是同一个程序。
- 两个或两个以上并存的进程还可以共享许多诸如打开的文件、地址空间之类的资源。



进程和线程是程序运行时状态，是动态变化的，进程和线程的管理操作(比如，创建，销毁等)都由内核来实现的。

Linux中**不严格区分进程和线程**，对Linux而言线程不过是一种特殊的进程。

 

现代操作系统中，进程提供2种虚拟机制：虚拟处理器和虚拟内存

- 虚拟处理器给进程一种假象，让这些进程觉得自己在独享处理器。
- 虚拟内存让进程在分配和管理内存时觉得自己拥有整个系统的所有内存资源。

每个进程有独立的虚拟处理器和虚拟内存

**在进程中的各个线程之间可以共享虚拟内存，但每个都拥有各自的虚拟处理器。**

 

进程的创建与退出：

1. 进程在创建它的时刻开始存活，在 Linux 系统中，这通常是调用 fork()系统的结果，该系统调用通过复制一个现有进程来创建一个全新的进程。调用 fork()的进程称为父进程，新产生的进程称为子进程。在该调用结束时，在返回点这个相同位置上，父进程恢复执行，子进程开始执行。 **fork()系统调用从内核返回两次：一次回到父进程，另一次回到新产生的子进程。**

2. 创建新的进程都是为了立即执行新的、不同的程序，而接着调用 exec() 这组函数就可以创建新的地址空间，并把新的程序载入其中。在现代 Linux 内核中， fork()实际上是由clone()系统调用实现的。

3. 最终，程序通过 exit()系统调用退出执行。这个函数会终结进程并将其占用的资源释放掉。进程退出执行后被设置为僵死状态，直到它的父进程调用 wait()或 waitpid()为止。



内核中进程的信息主要保存在task_struct中(include/linux/sched.h)

进程标识PID和线程标识TID对于同一个进程或线程来说都是相等的。

Linux中可以用ps命令查看所有进程的信息：

```sh
ps -eo pid,tid,ppid,comm
```

 

## 2. 进程描述符及任务结构

内核把进程的列表存放在叫做**任务队列** (task list) 的**双向循环链表**中。链表中的每一项都是类型为 task_struct 称为**进程描述符** (process descriptor) 的结构，该结构定义在 include/linux/sched.h文件中。进程描述符中包含一个具体进程的所有信息。

进程描述符中包含的数据能完整地描述一个正在执行的程序：它打开的文件，进程的地址空间，挂起的信号，进程的状态，等等。

<div align="center"> <img src="https://gitee.com//MrRen-sdhm/Images/raw/master/img/20200523212758.png" width="400px" /> </div> 



### 2.1 分配进程描述符

Linux 通过 **slab 分配器**分配 task_struct 结构，这样能达到对象复用和缓存着色 (cache coloring) 的目的。

在2.6版本之后用 slab 分配器动态生成 task_struct，所以只需在栈底（对于向下增长的栈来说）或栈顶（对于向上增长的栈来说）创建一个新的结构 struct thread_ info。

在 x86 上， struct thread_ info 在文件 <asm/tbread_info.b> 中定义如下：

```C
struct thread_info { 
    struct task_struct *task; 
    struct exec_domain *exec_domain; 
    __u32 flags; 
    __u32 status; 
    __u32 cpu; 
    int preempt_count; 
    mm_segment_t addr_limit; 
    struct restart_block restart_block; 
    void *sysenter_return; 
    int uaccess err; 
} ; 
```

<div align="center"> <img src="https://gitee.com//MrRen-sdhm/Images/raw/master/img/20200523213612.png" width="400px" />  </div> 

每个任务的 thread_info 结构在它的**内核栈的尾端**分配。结构中 task 域中存放的是指向该任务实际 task_struct 的指针。



### 2.2 进程描述符的存放

内核中大部分处理进程的代码都是直接通过 task_struct 进行的。因此，通过 current 宏查找到当前正在运行进程的进程描述符的速度就显得尤为重要。

硬件体系结构不同，该宏的实现也不同，它必须针对专门的硬件体系结构做处理。有的硬件体系结构可以拿出一个专门寄存器来存放指向当前进程 task_struct 的指针，用千加快访问速度。而有些像 x86 这样的体系结构（其寄存器并不富余），就只能在内核栈的尾端创建 thread_info 结构，通过计算偏移间接地查找 task_struct 结构。



### 2.3 进程状态

进程描述符中的 **state 域**描述了进程的当前状态 。系统中的每个进程都必然处于五种进程状态中的一种。

- TASK_RUNNING （**运行**）一 进程是可执行的；**它或者正在执行，或者在运行队列中等待执行**。这是进程在用户空间中执行的唯一可能的状态；这种状态也可以应用到内核空间中正在执行的进程。
- TASK_INTERRUPTIBLE （**可中断**）一 进程正在睡眠（也就是说它**被阻塞**），等待某些条件的达成。一且这些条件达成，内核就会把进程状态设置为运行。处千此状态的进程也会因为接收到信号而提前被唤醒并随时准备投入运行。
- TASK_ UNINTERRUPTIBLE （**不可中断**）一 除了**就算是接收到信号也不会被唤醒或准备投入运行**外，这个状态与可打断状态相同。这个状态通常在进程必须在等待时不受干扰或等待事件很快就会发生时出现。由于处于此状态的任务对信号不做响应，所以较之可中断状态 , 使用得较少。
- TASK_TRACED （**被跟踪**）— 被其他进程跟踪的进程，例如通过 ptrace 对调试程序进行跟踪。
- TASK_STOPPED （**停止**）— 进程停止执行；进程没有投入运行也不能投人运行。通常这种状态发生在接收到 SIGSTOP 、 SIGTSTP 、 SIGTTIN 、 SIGTTOU 等信号的时候。此外，在调试期间接收到任何信号，都会使进程进入这种状态。

<div align="center"> <img src="https://gitee.com//MrRen-sdhm/Images/raw/master/img/20200523211906.png" width="500px" /> </div>

进程的各个状态之间的转化构成了进程的整个生命周期。



### 2.4 设置当前进程状态

内核经常需要调整某个进程的状态。这时最好使用 set_task_state(task, state) 函数：

```C
set_task_state(task, state); /*将任务task的状态设置为state*/
```

该函数将指定的进程设置为指定的状态。必要的时候，它会设置内存屏障来强制其他处理器作重新排序。否则，它等价于：

```C
task->state = state;
```



### 2.5 进程上下文

一般程序在用户空间执行。**当一个程序执行了系统调用或者触发了某个异常，它就陷入了内核空间。此时，我们称内核「代表进程执行」并处于进程上下文中**。在此上下文中 current 宏是有效的。除非在此间隙有更高优先级的进程需要执行并由调度器做出了相应调整，否则在内核退出的时候，程序恢复在用户空间会继续执行。

系统调用和异常处理程序是对内核明确定义的接口。进程只有通过这些接口才能陷入内核执行——对内核的所有访间都必须通过这些接口。



### 2.6 进程家族树

 Linux 系统中进程之间存在一个明显的继承关系，**所有的进程都是 PID 为 1 的 init 进程的后代**。**内核在系统启动的最后阶段启动 init 进程**。

init 进程读取系统的初始化脚本 (initscript) 并执行其他的相关程序，最终完成系统启动的整个过程。

系统中的每个进程必有一个父进程，相应的，每个进程也可以拥有零个或多个子进程，每个 task_struct 都
包含一个指向其父进程 tast_struct 的 **parent 指针**，还包含一个称为 **children 的子进程链表**。

init 进程的进程描述符是作为 init_task 静态分配的。



## 3. 进程的创建

Linux中创建进程与其他系统有个主要区别，**Linux中创建进程分2步：fork() 和 exec()**，而其他系统通常提供spawn() 函数创建进程并读入可执行文件，然后开始执行。

- fork: 通过拷贝当前进程创建一个子进程

- exec: 读取可执行文件，将其载入到内存中运行
  - exec() 在这里指所有 exec() 一族的函数。内核实现了 execve() 函数，在此基础上，还实现了 execlp()、 execle()、execv() 和 execvp()。



### 3.1 写时拷贝

传统 fork() 系统调用：直接把所有的资源复制给新创建的进程。这种实现过于简单并且效率低下，因为它拷贝的数据也许并不共享，更糟的情况是，如果新进程打算立即执行一个新的映像，那么所有的拷贝都将前功尽弃。

**Linux 的 fork() 使用写时拷贝 (copy-on-write) 页实现**：

- 写时拷贝是一种可以推迟甚至免除拷贝数据的技术。内核此时并不复制整个进程地址空间，而是让父进程和子进程共享同一个拷贝。
- 只有在需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝。资源的复制只有在需要写入的时候才进行，在此之前，只是以只读方式共享
- 这种技术使地址空间上的页的拷贝被推迟到实际发生写入的时候才进行。在页根本不会被写入的情况下（举例来说，fork() 后立即调用 exec() 它们就无须复制了。

**fork() 的实际开销就是复制父进程的页表以及给子进程创建唯一的进程描述符**。在一般情况下，进程创建后都会马上运行一个可执行的文件，**这种优化可以避免拷贝大量根本就不会被使用的数据**（地址空间里常常包含数十兆的数据）。由于 Unix 强调进程快速执行的能力，所以这个优化是很重要的。



### 3.2 fork()

**Linux 通过 clone() 系统调用实现 fork()**。这个调用通过一系列的参数标志来指明父、子进程需要共享的资源。 fork()、 vfork()和＿clone() 库函数都根据各自需要的参数标志去调用 clone()，然后由 clone() 去调用 do_fork()。

do_fork() 完成了创建中的大部分工作，它的定义在 kernel/fork.c 文件中。该函数调用 copy_process() 函数，然后让进程开始运行。 copy_process() 函数完成的工作如下：

1. 调用dup_task_struct() 为新进程分配内核栈、thread_info 和 task_struct 等，其中的内容与父进程相同。此时，子进程和父进程的描述符是完全相同的。
2. check新进程（进程数目是否超出上限等）
3. 清理新进程的信息（比如PID置0等），使之与父进程区别开。
4. 新进程状态置为 TASK_UNINTERRUPTIBLE，以保证它不会投入运行。
5. 调用 copy_ftags() 更新 task_struct 的 flags 成员。
6. 调用 alloc_pid() 为新进程分配一个有效的 PID
7. 根据clone()的参数标志，拷贝或共享相应的信息
8. 做一些扫尾工作并返回指向新进程的指针

copy_process() 函数执行完成后，返回到do_fork() 函数。若copy_process() 函数成功返回，新创建的子进程被唤醒并让其投入运行。

**内核有意选择子进程首先执行 。因为一般子进程都会马上调用 exec() 函数，这样可以避免写时拷贝的额外开销。**

**创建进程的fork()函数实际上最终是调用clone()函数。**



### 3.3 vfork()

除了**不拷贝父进程的页表项**外， vfork() 系统调用和 fork() 的功能相同。子进程作为父进程的一个单独的线程在它的地址空间里运行，父进程被阻塞，**直到子进程退出或执行 exec()**。子进程不能向地址空间写入。

现在由于在执行 fork() 时引入了写时拷贝页并且明确了子进程先执行，vfork() 的好处就**仅限于不拷贝父进程的页表项**了。如果 Linux 将来 fork() 有了写时拷贝页表项，那么vfork() 就彻底没用了。

vfork() 系统调用的实现是通过向 clone() 系统调用传递一个特殊标志来进行的。



## 4 线程在 Linux 中的实现

Linux 实现线程的机制非常独特。从内核的角度来说，它并没有线程这个概念。 Linux 把所有的线程都当做进程来实现。内核并没有准备特别的调度算法或是定义特别的数据结构来表征线程。**线程仅仅被视为一个与其他进程共享某些资源的进程**。**每个线程都拥有唯一隶属于自己的 task_struct**，所以在内核中，它看起来就像是一个普通的进程（只是线程和其他一些进程共享某些资源，如地址空间）。

「轻最级进程」这种叫法本身就概括了 Linux 在此处与其他系统的差异：

- 在其他的系统中，相较于重量级的进程，线程被抽象成一种耗费较少资源，运行迅速的执行单元。
- 而对于 Linux 来说，它只是一种进程间共享资源的手段。



### 4.1 创建线程

创建线程和进程的步骤一样，只是最终传给clone()函数的参数不同。

比如，通过一个普通的fork来创建进程，相当于：`clone(SIGCHLD, 0)`

创建一个和父进程共享地址空间，文件系统资源，文件描述符和信号处理程序的进程，即一个线程：`clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0)`

传递给 clone() 的参数标志决定了新创建进程的行为方式和父子进程之间共享的资源种类。这些参数标志在 <linux/sched.h> 中定义。



### 4.2 内核线程

在内核中创建的内核线程与普通的进程之间的区别在于：**内核线程没有独立的地址空间，它们只能在内核空间运行**，从来不切换到用户空间去。内核进程和普通进程一样，可以被调度，也可以被抢占。

内核线程也只能由其他内核线程创建，内核是通过从 kthreadd 内核进程中衍生出所有新的内核线程来自动处理这一点的。新的任务是由 kthread 内核进程通过 clone() 系统调用而创建的。

它们只在内核空间运行，从来不切换到用户空间去。内核进程和普通进程一样，可以被调度，也可以被抢占。

 

## 5. 进程的终止

### 5.1 删除进程描述符

进程的终止靠 do_exit() （定义于kemel/exit.c) 来完成，和创建进程一样，终结一个进程同样有很多步骤：

1、子进程上的操作(do_exit)

1. 设置task_struct中的标识成员设置为PF_EXITING
2. 调用del_timer_sync() 删除内核定时器, 确保没有定时器在排队和运行
3. 调用exit_mm()释放进程占用的mm_struct
4. 调用sem__exit()，使进程离开等待IPC信号的队列
5. 调用exit_files() 和exit_fs()，释放进程占用的文件描述符和文件系统资源
6. 把task_struct的exit_code设置为进程的返回值
7. 调用exit_notify() 向父进程发送信号，并把自己的状态设为EXIT_ZOMBIE
8. 切换到新进程继续执行

子进程进入EXIT_ZOMBIE之后，虽然永远不会被调度，关联的资源也释放掉了，但是它本身占用的内存还没有释放，比如创建时分配的内核栈、thread_info及task_struct结构等。这些由父进程来释放。此时进程存在的唯一目的就是向它的父进程提供信息。父进程检索到信息后，或者通知内核那是无关的信息后，由进程所持有的剩余内
存被释放，归还给系统使用。



在调用了 do_exit() 之后，尽管线程已经僵死不能再运行了，但是**系统还保留了它的进程描述符**。前面说过，这样做可以让系统有办法在子进程终结后仍能获得它的信息。因此，**进程终结时所需的清理工作和进程描述符的删除被分开执行**。在父进程获得已终结的子进程的信息后，或者通知内核它并不关注那些信息后，子进程的 task_struct 结构才被释放。



2、父进程上的操作(release_task)

父进程收到子进程发送的exit_notify()信号后，将该子进程的进程描述符和所有进程独享的资源全部删除。

1. 它调用\_exit_signal(），该函数调用\_unhash\_process(），后者又调用 detach_pid() 从 pidhash
   上删除该进程，同时也要从任务列表中删除该进程。
2. \_exit_signal() 释放目前僵死进程所使用的所有剩余资源，并进行最终统计和记录。
3. 如果这个进程是线程组最后一个进程，并且领头进程已经死掉，那么 release_task() 就要通知僵死的领头进程的父进程。
4. release_task()调用 put_task_struct() 释放进程内核栈和 thread_info 结构所占的页，并释放tast_struct 所占的 slab 高速缓存。

至此，进程描述符和所有进程独享的资源就全部释放掉了。



### 5.2 孤儿进程造成的进退维谷

如果父进程在子进程之前退出，必须有机制来保证子进程能找到一个新的父亲，否则这些成为孤儿的进程就会在退出时永远处于僵死状态，白白地耗费内存。

解决方法是**给子进程在当前线程组内找一个线程作为父亲，如果不行，就让 init 做它们的父进程**。在 do_exit(）中会调用 exit_notify()，该函数会调用 forget_original__parent()，而后者会调用 find_new _reaper() 来执行寻父过程。

find_new_reaper()函数先在当前线程组中找一个线程作为父亲，如果找不到，就让init做父进程。(init进程是在linux启动时就一直存在的)

一旦系统为进程成功地找到和设置了新的父进程，就不会再有出现驻留僵死进程的危险了。init 进程会例行调用 wait( ）来检查其子进程，清除所有与其相关的僵死进程。



# 第四章 进程调度

调度程序负责**决定将哪个进程投入运行，何时运行以及运行多长时间**。进程调度程序（常常简称调度程序）可看做**在可运行态进程之间分配有限的处理器时间资源的内核子系统**。调度程序是像 Linux 这样的多任务操作系统的基础。只有通过调度程序的合理调度，系统资源才能最大限度地发挥作用，多进程才会有并发执行的效果。

最大限度地利用处理器时间的原则是，**只要有可以执行的进程，那么就总会有进程正在执行**。但是只要系统中可运行的进程的数目比处理器的个数多，就注定某一给定时刻会有一些进程不能执行。这些进程在**等待运行**。**在一组处于可运行状态的进程中选择一个来执行，是调度程序所需完成的基本工作**。



## 1. 多任务

 多任务操作系统就是能同时并发地交互执行多个进程的操作系统。在单处理器机器上，这会产生多个进程在同时运行的幻觉。在多处理器机器上，这会使多个进程在不同的处理机上真正同时、并行地运行。

无论在单处理器或者多处理器机器上，多任务操作系统都能使多个进程处于堵塞或者睡眠状态，也就是说，实际上不被投入执行，直到工作确实就绪。这些任务尽管位于内存，但并不处于可运行状态。相反，这些进程利用内核阻塞自己，直到某一事件（键盘输人、网
络数据、过一段时间等）发生。

多任务系统可以划分为两类：

1. 非抢占式多任务

2. 抢占式多任务

**Linux 提供了抢占式的多任务模式**。在此模式下，**由调度程序来决定什么时候停止一个进程的运行，以便其他进程能够得到执行机会**。这个强制的挂起动作就叫做**抢占** (preemption) 。进程在被抢占之前能够运行的时间是预先设置好的，而且有一个专门的名字，叫进程的**时间片** (timeslice) 。时间片实际上就是分配给每个可运行进程的处理器时间段。有效管理时间片能使调度程序从系统全局的角度做出调度决定，这样做还可以避免个别进程独占系统资源。当今众多现代操作系统对程序运行都采用了动态时间片计算的方式，并且引入了可配置的计算策略。

在非抢占式多任务模式下，除非进程自己主动停止运行，否则它会一直执行。进程主动挂起自己的操作称为**让步** (yielding)。理想情况下，进程通常做出让步，以便让每个可运行进程享有足够的处理器时间。但这种机制**有很多缺点**：调度程序无法对每个进程该执行多长时间做出统一规定，所以进程独占的处理器时间可能超出用户的预料；更糟的是，一个决不做出让步的悬挂进程就能使系统崩溃。



## 2. Linux的进程调度

Linux 2.5 引入 O(1) 调度程序，其使用了静态时间片算法和针对每一处理器的运行队列。

Linux 2.6 引入了新的进程调度算法。其中最为著名的是 “反转楼梯最后期限调度算法“（RSDL），该算法吸取了队列理论，将公平调度的概念引入了 Linux 调度程序。并且最终在 2.6.23 内核版本中替代了 O(1) 调度算法，它此刻被称为 “**完全公平调度算法**”，或者简称 CFS。



## 3. 策略

**策略决定调度程序在何时让什么进程运行**。调度器的策略往往就决定系统的整体印象，并且还负责优化使用处理器时间，是至关重要的。



### 3.1 I/O 消耗型和处理器消耗型的进程

**进程可以被分为 I/O 消耗型和处理器消耗型**：

- I/O 消耗型指进程的**大部分时间用来提交 I/O 请求或是等待 I/O 请求**。因此，这样的进程经常处于可运行状态，但通常都是运行短短的一会儿，因为它在等待更多的 I/O 请求时最后总会阻塞。
- 处理器耗费型进程把**时间大多用在执行代码上**。除非被抢占，否则它们通常都一直不停地运行，因为它们没有太多的 1/0 需求。但是，因为它们不属千 1/0 驱动类型，所以从系统响应速度考虑，调度器不应该经常让它们运行。对千这类处理器消耗型的进程，调度策略往往是尽量降低它们的调度频率，而延长其运行时间。处理器消耗型进程的极端例子就是无限循环地执行。

当然，这种划分方法并非是绝对的。**进程可以同时展示这两种行为**：比如， X Window 服务器既是 I/O 消耗型，也是处理器消耗型。还有些进程可以是 I/O 消耗型，但属于处理器消耗型活动的范围。其典型的例子就是字处理器，其通常坐以等待键盘输入，但在任一时刻可能又粘住处理器疯狂地进行拼写检查或者宏计算。

调度策略通常要在两个矛盾的目标中间寻找平衡：**进程响应迅速（响应时间短）和最大系统利用率（高吞吐量）。**为了满足上述需求，调度程序通常采用一套非常复杂的算法来决定最值得运行的进程投入运行，但是它**往往并不保证低优先级进程会被公平对待**。Linux 为了保证交互式应用和桌面系统的性能，所以对进程的响应做了优化（缩短响应时间），**更倾向于优先调度 I/O 消耗型进程**。虽然如此，但在下面你会看到，调度程序也并未忽略处理器消耗型的进程。



### 3.2 进程优先级（此处开始未作笔记）

前面说过，调度功能就是决定哪个进程运行以及进程运行多长时间。

决定哪个进程运行以及运行多长时间都和进程的优先级有关。为了确定一个进程到底能持续运行多长时间，调度中还引入了时间片的概念。



进程的优先级有2种度量方法，一种是nice值，一种是实时优先级。

nice值的范围是-20～+19，值越大优先级越低，也就是说nice值为-20的进程优先级最大。

实时优先级的范围是0～99，与nice值的定义相反，实时优先级是值越大优先级越高。

实时进程都是一些对响应时间要求比较高的进程，因此系统中有实时优先级高的进程处于运行队列的话，它们会抢占一般的进程的运行时间。

 

进程的2种优先级会让人不好理解，到底哪个优先级更优先？一个进程同时有2种优先级怎么办？

其实linux的内核早就有了解决办法。

对于第一个问题，到底哪个优先级更优先？

答案是实时优先级高于nice值，在内核中，实时优先级的范围是 0～MAX_RT_PRIO-1 MAX_RT_PRIO的定义参见 include/linux/sched.h

```
1611 #define MAX_USER_RT_PRIO        100
1612 #define MAX_RT_PRIO             MAX_USER_RT_PRIO
```

nice值在内核中的范围是 MAX_RT_PRIO～MAX_RT_PRIO+40 即 MAX_RT_PRIO～MAX_PRIO

```
1614 #define MAX_PRIO                (MAX_RT_PRIO + 40)
```

 

第二个问题，一个进程同时有2种优先级怎么办？

答案很简单，就是一个进程不可能有2个优先级。一个进程有了实时优先级就没有Nice值，有了Nice值就没有实时优先级。

我们可以通过以下命令查看进程的实时优先级和Nice值：(其中RTPRIO是实时优先级，NI是Nice值)

```
$ ps -eo state,uid,pid,ppid,rtprio,ni,time,comm
S   UID   PID  PPID RTPRIO  NI     TIME COMMAND
S     0     1     0      -   0 00:00:00 systemd
S     0     2     0      -   0 00:00:00 kthreadd
S     0     3     2      -   0 00:00:00 ksoftirqd/0
S     0     6     2     99   - 00:00:00 migration/0
S     0     7     2     99   - 00:00:00 watchdog/0
S     0     8     2     99   - 00:00:00 migration/1
S     0    10     2      -   0 00:00:00 ksoftirqd/1
S     0    12     2     99   - 00:00:00 watchdog/1
S     0    13     2     99   - 00:00:00 migration/2
S     0    15     2      -   0 00:00:00 ksoftirqd/2
S     0    16     2     99   - 00:00:00 watchdog/2
S     0    17     2     99   - 00:00:00 migration/3
S     0    19     2      -   0 00:00:00 ksoftirqd/3
S     0    20     2     99   - 00:00:00 watchdog/3
S     0    21     2      - -20 00:00:00 cpuset
S     0    22     2      - -20 00:00:00 khelper
```

 

### 3.3 时间片

有了优先级，可以决定谁先运行了。但是对于调度程序来说，并不是运行一次就结束了，还必须知道间隔多久进行下次调度。

于是就有了时间片的概念。时间片是一个数值，表示一个进程被抢占前能持续运行的时间。

也可以认为是进程在下次调度发生前运行的时间(除非进程主动放弃CPU，或者有实时进程来抢占CPU)。

时间片的大小设置并不简单，设大了，系统响应变慢(调度周期长)；设小了，进程频繁切换带来的处理器消耗。默认的时间片一般是10ms

 

## 2.3 调度实现原理（基于优先级和时间片）

下面举个直观的例子来说明：

假设系统中只有3个进程ProcessA(NI=+10)，ProcessB(NI=0)，ProcessC(NI=-10)，NI表示进程的nice值，时间片=10ms

1. 调度前，把进程优先级按一定的权重映射成时间片(这里假设优先级高一级相当于多5msCPU时间)。

   假设ProcessA分配了一个时间片10ms，那么ProcessB的优先级比ProcessA高10(nice值越小优先级越高)，ProcessB应该分配10\*5+10=60ms，以此类推，ProcessC分配20\*5+10=110ms

2. 开始调度时，优先调度分配CPU时间多的进程。由于ProcessA(10ms),ProcessB(60ms),ProcessC(110ms)。显然先调度ProcessC

3. 10ms(一个时间片)后，再次调度时，ProcessA(10ms),ProcessB(60ms),ProcessC(100ms)。ProcessC刚运行了10ms，所以变成100ms。此时仍然先调度ProcessC

4. 再调度4次后(4个时间片)，ProcessA(10ms),ProcessB(60ms),ProcessC(60ms)。此时ProcessB和ProcessC的CPU时间一样，这时得看ProcessB和ProcessC谁在CPU运行队列的前面，假设ProcessB在前面，则调度ProcessB

5. 10ms(一个时间片)后，ProcessA(10ms),ProcessB(50ms),ProcessC(60ms)。再次调度ProcessC

6. ProcessB和ProcessC交替运行，直至ProcessA(10ms),ProcessB(10ms),ProcessC(10ms)。

     这时得看ProcessA，ProcessB，ProcessC谁在CPU运行队列的前面就先调度谁。这里假设调度ProcessA

7. 10ms(一个时间片)后，ProcessA(时间片用完后退出),ProcessB(10ms),ProcessC(10ms)。

8. 再过2个时间片，ProcessB和ProcessC也运行完退出。



这个例子很简单，主要是为了说明调度的原理，实际的调度算法虽然不会这么简单，但是基本的实现原理也是类似的：

1. 确定每个进程能占用多少CPU时间(这里确定CPU时间的算法有很多，根据不同的需求会不一样)

2. 占用CPU时间多的先运行

3. 运行完后，扣除运行进程的CPU时间，再回到 1）

 

## 3. Linux上调度实现的方法

Linux上的调度算法是不断发展的，在2.6.23内核以后，采用了“完全公平调度算法”，简称CFS。

CFS算法在分配每个进程的CPU时间时，不是分配给它们一个绝对的CPU时间，而是根据进程的优先级分配给它们一个占用CPU时间的百分比。

比如ProcessA(NI=1)，ProcessB(NI=3)，ProcessC(NI=6)，在CFS算法中，分别占用CPU的百分比为：ProcessA(10%)，ProcessB(30%)，ProcessC(60%)

因为总共是100%，ProcessB的优先级是ProcessA的3倍，ProcessC的优先级是ProcessA的6倍。

 

Linux上的CFS算法主要有以下步骤：(还是以ProcessA(10%)，ProcessB(30%)，ProcessC(60%)为例)

1. 计算每个进程的vruntime(注1)，通过update_curr()函数更新进程的vruntime。

2. 选择具有最小vruntime的进程投入运行。（注2）

3. 进程运行完后，更新进程的vruntime，转入步骤2) （注3）

 

**注1.** 这里的vruntime是进程虚拟运行的时间的总和。vruntime定义在：kernel/sched_fair.c 文件的 struct sched_entity 中。

 

**注2.** 这里有点不好理解，根据vruntime来选择要运行的进程，似乎和每个进程所占的CPU时间百分比没有关系了。

1. 比如先运行ProcessC，(vr是vruntime的缩写)，则10ms后：ProcessA(vr=0)，ProcessB(vr=0)，ProcessC(vr=10)

2. 那么下次调度只能运行ProcessA或者ProcessB。(因为会选择具有最小vruntime的进程)

长时间来看的话，ProcessA、ProcessB、ProcessC是公平的交替运行的，和优先级没有关系。

而实际上**vruntime**并不是实际的运行时间，它是**实际运行时间进行加权运算**后的结果。

比如上面3个进程中ProcessA(10%)只分配了CPU总的处理时间的10%，那么ProcessA运行10ms的话，它的vruntime会增加100ms。

以此类推，ProcessB运行10ms的话，它的vruntime会增加(100/3)ms,ProcessC运行10ms的话，它的vruntime会增加(100/6)ms。

实际的运行时，由于ProcessC的vruntime增加的最慢，所以它会获得最多的CPU处理时间。

上面的加权算法是我自己为了理解方便简化的，Linux对vruntime的加权方法还得去看源码^-^

 

**注3.**Linux为了能快速的找到具有最小vruntime，将所有的进程的存储在一个红黑树中。这样树的最左边的叶子节点就是具有最小vruntime的进程，新的进程加入或有旧的进程退出时都会更新这棵树。

 

其实Linux上的调度器是以模块方式提供的，每个调度器有不同的优先级，所以可以同时存在多种调度算法。

每个进程可以选择自己的调度器，Linux调度时，首先按调度器的优先级选择一个调度器，再选择这个调度器下的进程。

 

## 4. 调度相关的系统调用

调度相关的系统调用主要有2类：

1) 与调度策略和进程优先级相关 (就是上面的提到的各种参数，优先级，时间片等等) - 下表中的前8个

2) 与处理器相关 - 下表中的最后3个

| **系统调用**             | **描述**                                                     |
| ------------------------ | ------------------------------------------------------------ |
| nice()                   | 设置进程的nice值                                             |
| sched_setscheduler()     | 设置进程的调度策略，即设置进程采取何种调度算法               |
| sched_getscheduler()     | 获取进程的调度算法                                           |
| sched_setparam()         | 设置进程的实时优先级                                         |
| sched_getparam()         | 获取进程的实时优先级                                         |
| sched_get_priority_max() | 获取实时优先级的最大值，由于用户权限的问题，非root用户并不能设置实时优先级为99 |
| sched_get_priority_min() | 获取实时优先级的最小值，理由与上面类似                       |
| sched_rr_get_interval()  | 获取进程的时间片                                             |
| sched_setaffinity()      | 设置进程的处理亲和力，其实就是保存在task_struct中的cpu_allowed这个掩码标志。该掩码的每一位对应一个系统中可用的处理器，默认所有位都被设置，即该进程可以再系统中所有处理器上执行。用户可以通过此函数设置不同的掩码，使得进程只能在系统中某一个或某几个处理器上运行。 |
| sched_getaffinity()      | 获取进程的处理亲和力                                         |
| sched_yield()            | 暂时让出处理器                                               |



# 第七章 中断和中断处理

中断机制：硬件在需要的时候主动向内核发出信号

## 1. 中断

中断是一种电信号，由硬件设备生成，并直接送入中断控制器的输入引脚中，不同的设备对应的中断不同，而每个中断都通过一个唯一的数字标志。

异常：异常与中断不同，它在产生时必须考虑与处理器**时钟同步**。实际上，异常也常常称为同步中断。在处理器执行到**由于编程失误而导致的错误指令**（如被 0 除）的时候，或者是在**执行期间出现特殊情况**（如缺页），必须靠内核来处理的时候，处理器就会产生一个异常，因为许多处理器体系结构处理异常与处理中断的方式类似，因此，内核对它们的处理也很类似。

**「中断与异常的区别」**：**中断在硬件设备需要时主动向内核发送信号，而异常是由处理器本身产生的同步中断**。

一个使用中断和异常的例子：系统调用通过软中断实现，即陷入内核，然后引起一种特殊的异常——系统调用处理程序异常。



## 2. 中断处理程序

在响应一个特定中断的时候，内核会执行一个函数，该函数叫做中断处理程序 (interrupt handler) 或中断服务例程 (interrupt service routine, ISR)

**中断处理程序与其他内核函数的区别在于**，中断处理程序是**被内核调用**来响应中断的，而它们运行于称之为**中断上下文**的特殊上下文中。

中断上下文也称作原子上下文，该上下文中的执行代码不可阻塞。

**Linux 中的中断处理程序是无须重入的**。当一个给定的中断处理程序正在执行时，相应的中断线在所有处理器上都会被屏蔽掉，以防止在同一中断线上接收另一个新的中断。通常情况下，所有其他的中断都是打开的，所以这些不同中断线上的其他中断都能被处理，但当前中断线总是被禁止的。由此可以看出，**同一个中断处理程序绝对不会被同时调用以处理嵌套的中断**。这极大地简化了中断处理程序的编写。



## 3. 上半部与下半部

我们希望中断处理程序运行得快，并且能够完成的工作量多。为了同时解决这两个问题，中断处理一般分为两个部分。

- **中断处理程序是上半部**(top half)，一接收到一个中断，它就立即开始执行，但只做有严格时限的工作。
- **允许稍后完成的工作会推迟到下半部** (bottom half) 去。此后，在合适的时机，下半部会被开中断执行。



## 4. 中断上下文

当执行一个中断处理程序时，内核处于中断上下文 (interrput context) 中。

中断上下文具有较为严格的时间限制，因为它打断了其他代码。中断上下文中的代码应当迅速、简洁，尽量不要使用循环去处理繁重的工作。**中断处理程序打断了其他的代码（甚至可能是打断了在其他中断线上的另一中断处理程序）**。正是因为这种异步执行的特性，所以所有的中断处理程序必须尽可能的迅速、简洁。尽量把工作从中断处理程序中分离出来，放在下半部来执行，因为下半部可以在更合适的时间运行。



# 第八章  中断下半部和推后执行的工作

由于中断处理程序存在一些局限，所以它只能完成整个中断处理流程的上半部分。这些局限包括：

- 中断处理程序以异步方式执行，并且它有可能会打断其他重要代码。为了**避免被打断的代码停止时间过长**，中断处理程序应该执行得越快越好。
- 如果当前有一个中断处理程序正在执行，在最好的情况下与该中断同级的其他中断会被屏蔽，在最坏的情况下，当前处理器上所有其他中断都会被屏蔽。**禁止中断后硬件与操作系统无法通信**，因此，中断处理程序执行得越快越好。
- 由于中断处理程序往往需要对硬件进行操作，所以它们通常有很高的时限要求。
- 中断处理程序不在进程上下文中运行，所以它们不能阻塞。



## 1.  下半部

**下半部的任务就是执行与中断处理密切相关但中断处理程序本身不执行的工作**。在理想的情况下，最好是中断处理程序将所有工作都交给下半部分执行，因为我们希望在中断处理程序中完成的工作越少越好（也就是越快越好）。我们期望中断处理程序能够尽可能快地返回。

对于上半部和下半部划分的建议：

- 如果一个任务对时间非常敏感，将其放在中断处理程序中执行
- 如果一个任务和硬件相关，将其放在中断处理程序中执行
- 如果一个任务要保证不被其他中断（特别是相同的中断）打断，将其放在中断处理程序中执行
- 其他所有任务，考虑放置在下半部执行

为什么要用下半部：

- 中断处理程序在运行的时候，**当前的中断线在所有处理器上都会被屏蔽**

- 如果一个处理程序是 IRQF_ DISABLED 类型，它执行的时候会**禁止所有本地中断（而且把本地中断线全局地屏蔽掉）**

- **缩短中断被屏蔽的时间**对系统的响应能力和性能都至关重要
- 中断处理程序要与其他程序（甚至是其他的中断处理程序）**异步**执行

通常下半部在中断处理程序一返回就会马上运行。下半部执行的关键在于**当下半部运行的时候，允许响应所有的中断**。

下半部的实现机制：

- 2.5版本之前：
  - BH（bottom half）机制
  - 任务队列

- **2.5版本之后**：
  - 软中断：一组**静态定义**的下半部接口，有 32 个，**两个相同类型的软中断可以同时被执行**
  - tasklet：一种基于软中断实现的灵活性强、**动态创建**的下半部实现机制。两个不同类型的 tasklet 可以在不同的处理器上同时执行，但**相同类型的 tasklet 不能同时执行**。tasklet 基于软中断实现。
  - 工作队列：一种简单但很有用的方法，先对要推后执行的工作排队，稍后在进程上下文中执行它们

软中断必须在编译期间就进行静态注册。与此相反， tasklet 可以通过代码进行动态注册。

另外一个可以用于将工作推后执行的机制是**内核定时器**，可以保证在一个确定的时间段过去以后再运行



## 2. 软中断

软中断保留给系统中**对时间要求最严格以及最重要的**下半部使用。目前，只有两个子系统（网络和 SCSI) 直接使用软中断。此外，内核定时器和 tasklet 都是建立在软中断上的。对于时间要求严格并能自己高效地完成加锁工作的应用，软中断会是正确的选择。



## 3. tasklet

tasklet 是利用软中断实现的一种下半部机制，它和进程没有任何关系。 tasklet 和软中断在本质上很相似，行为表现也相近，但是它的接口更简单，对锁保护的要求也较低。

选择标准：**通常你应该用 tasklet**。就像我们在前面看到的，软中断的使用者屈指可数。它只在那些执行频率很高和连续性要求很高的情况下才需要使用。而 tasklet 却有更广泛的用途。大多数情况下用 tasklet 效果都不错，而且它们还非常容易使用。

tasklet 的实现：

因为 tasklet 是通过软中断实现的，所以它们本身也是软中断。tasklet 由两类软中断代表： HI_SOFTIRQ 和 TASKLET_SOFTIRQ 。这两者之间唯一的实际区别在于， HI_SOFTIRQ 类型的软中断先于 TASKLET_SOFTIRQ 类型的软中断执行。

- tasklet 结构体：tasklet 由 tasklet_ struct 结构表示。每个结构体单独代表一个 tasklet
- 调度 tasklet：已调度的 tasklet （等同于被触发的软中断）存放在两个单处理器数据结构： tasklet_vec（普通 tasklet) 和 tasklet_hi_ vec（高优先级的 tasklet) 。这两个数据结构都是由 tasklet_struct 结构体构成的链表。链表中的每个 tasklet_struct 代表一个不同的 tasklet 。
  - tasklet 由 tasklet_schedule() 和 tasklet_hi_ schedule() 函数进行调度，它们接受一个指向 tasklet_struct 结构的指针作为参数，区别在于一个使用 TASKLET_ SOFTIRQ 而另一个用 HI_SOFTIRQ

tasklet 的实现很简单，但非常巧妙。我们可以看到，**所有的 tasklet 都通过重复运用 HI_SOFTIRQ 和 TASKLET_SOFTIRQ 这两个软中断实现**。当一个 tasklet 被调度时，内核就会唤起这两个软中断中的一个。随后，该软中断会**被特定的函数处理**，执行所有已调度的 tasklet。这个函数**保证同一时间里只有一个给定类别的 tasklet 会被执行**（但其他不同类型的 tasklet 可以同时执行）。所有这些复杂性都被一个简洁的接口隐藏起来了。

tasklet 的使用：

大多数情况下，为了控制一个寻常的硬件设备， tasklet 机制都是实现自己的下半部的最佳选择。 tasklet 可以动态创建，使用方便，执行起来也还算快。



## 4. 工作队列

工作队列 (work queue) 是另外一种将工作推后执行的形式，它和前面讨论的所有其他形式都不相同。工作队列可以把工作推后，交由一个**内核线程**去执行——这个下半部分总是会在**进程上下文**中执行。这样，通过工作队列执行的代码能占尽进程上下文的所有优势。最重要的就是工作队列**允许重新调度甚至是睡眠**。

在工作队列和软中断／tasklet 中做出选择非常容易：如果推后执行的任务**需要睡眠**，那么就**选择工作队列**。如果推后执行的任务不需要睡眠，那么就选择软中断或 tasklet

如果你需要用一个**可以重新调度的实体**来执行你的下半部处理，你应该使用工作队列。它是唯一**能在进程上下文中运行**的下半部实现机制，也只有它才可以睡眠。需要获得大量的内存时，在你需要获取信号量时，在你需要执行阻塞式的 I/O 操作时，它都会非常有用。

工作队列的实现：

工作队列子系统是一个**用于创建内核线程的接口**，通过它创建的进程**负责执行由内核其他部分排到队列里的任务**。它创建的这些内核线程称作**工作者线程** 。**工作队列可以让你的驱动程序创建一个专门的工作者线程来处理需要推后的工作**。不过，工作队列子系统提供了一个**缺省的工作者线程**来处理这些工作。因此，工作队列最基本的表现形式，就转变成了一个**把需要推后执行的任务交给特定的通用线程的一种接口**。

除非一个驱动程序或者子系统必须建立一个属于它自己的内核线程，否则最好使用缺省线程。但如果你需要在工作者线程中**执行大量的处理操作**，创建属于自己的工作者线程或许会带来好处。处理器密集型和性能要求严格的任务会因为拥有自己的工作者线程而获得好处。这么做也有助于减轻缺省线程的负担，避免工作队列中其他需要完成的工作处于饥饿状态。

- 表示线程的数据结构：工作者线程用 workqueue_ struct 结构体表示，是一个由 cpu_ workqueue_struct 结构组成的数组数组的每一项对应系统中的一个处理器

- 表示工作的数据结构：所有的工作者线程都是用**普通的内核线程**实现的，它们都要执行 worker_thread() 函数。在它初始化完以后，这个函数执行一个死循环并开始**休眠**。当有操作被插入到队列里的时候，线程就会被唤醒，以便执行这些操作。当没有剩余的操作时，它又会继续休眠。

  - 工作用 work_struct 结构体表示：这些结构体被连接成链表，在每个处理器上的每种类型的队列都对应这样一个链表。当一个工作者线程被唤醒时，它会执行它的链表上的所有工作。工作被执行完毕，它就将相应的 work_struct 对象从链表上移去。当链表上不再有对象的时候，它就会继续休眠。

- 工作队列实现机制：

  <img src="https://gitee.com//MrRen-sdhm/Images/raw/master/img/20200615105256.png" alt="image-20200615105234583" style="zoom: 80%;" />

  位于最高一层的是工作者线程。系统允许有多种类型的工作者线程存在。对于指定的一个类型，系统的每个 CPU 上都有一个该类的工作者线程。内核中有些部分可以根据需要来创建工作者线程，而在默认情况下内核只有 event 这一种类型的工作者线程。每个工作者线程都由一个 cpu_workequeue_ struct 结构体表示。而 workqueue_ struct 结构体则表示给定类型的所有工作者线程。
  
  大部分驱动程序都使用的是现存的默认工作者线程。它们使用起来简单、方便。可是，在有些要求更严格的情况下，驱动程序需要自己的工作者线程。

工作队列的使用：

  1. 创建推后的工作

     - 首先要做的是实际创建一些需要推后完成的工作。可以通过 DECLARE_WORK 在编译时静
       态地建该结构体：

       DECLARE_WORK(name, void (*func) (void *), void *data); 

       这样就会静态地创建一个名为 name, 处理函数为 func, 参数为 data 的 work—struct 结构体。

     - 同样，也可以在运行时通过指针创建一个工作：

       INIT_WORK(struct work_struct \*work, void(\*func) (void*), void *data); 

       这会动态地初始化一个由 work 指向的工作，处理函数为 func, 参数为 data 。

2. 工作队列处理函数

   工作队列处理函数的原型是：

   void work_handler(void *data)

   这个函数会由一个工作者线程执行，因此，函数会运行在进程上下文中。默认情况下，允许响应中断，并且不持有任何锁。如果需要，函数可以睡眠。需要注意的是，**尽管操作处理函数运行在进程上下文中，但它不能访问用户空间，因为内核线程在用户空间没有相关的内存映射**。通常在发生系统调用时，**内核会代表用户空间的进程运行**，此时它才能访问用户空间，也只有在此时它才会映射用户空间的内存。

   在工作队列和内核其他部分之间使用锁机制就像在其他的进程上下文中使用锁机制一样方便。

3. 对工作进行调度

   想要把给定工作的处理函数提交给缺省的 events工作线程，只需调用：

   schedule_work (&work) ;

   work 马上就会被调度，一旦其所在的处理器上的工作者线程被唤醒，它就会被执行。若希望它经过一段延迟以后再执行。在这种情况下，可以调度它在指定的时间执行：

   schedule_delayed_work(&work, delay); 

   这时，＆work 指向的 work_struct 直到 delay 指定的时钟节拍用完以后才会执行。

4. 刷新操作

   排入队列的工作会在工作者线程下一次被唤醒的时候执行。有时，在继续下一步工作之前，你必须保证一些操作已经执行完毕了。内核提供了一个用于刷新指定工作队列的函数：

   void flush_scheduled_work(void); 

   函数会一直等待，直到队列中所有对象都被执行以后才返回。在等待所有待处理的工作执行的时候，该函数会进入休眠状态，所以只能在进程上下文中使用它。

   注意，该函数**并不取消任何延迟执行的工作**。就是说，任何通过 schedule_delayed_ work() 调度的工作，如果其延迟时间未结束，它并不会因为调用 flush_scheduled_ work() 而被刷新掉。取消延迟执行的工作应该调用：

   int cancel_delayed_work(struct work_struct *work);

   这个函数可以取消任何与 work_struct 相关的挂起工作。

5. 创建新的工作队列

   如果缺省的队列不能满足你的需要，你应该创建一个新的工作队列和与之相应的工作者线程。这么做**会在每个处理器上都创建一个工作者线程**。

   创建一个新的任务队列和与之相关的工作者线程：

   struct workqueue_struct *create_workqueue(const char *name) ; 

   这样就会创建所有的工作者线程（系统中的每个处理器都有一个），并且做好所有开始处理工作之前的准备工作。

   创建一个工作的时候无须考虑工作队列的类型。在创建之后，可以调用下面列举的函数。这些函数与 schedule_work() 以及 schedule_delayed_ work() 相近，唯一的区别就在于它们针对给定的工作队列而不是缺省的 events 队列进行操作。

   int queue_work(struct workqueue_struct *wq, struct work_struct *work) 

   int queue_delayed_work(struct workqueue_struct *wq, 
   											struct work_struct *work, 
   											unsigned long delay) 

   flush_workqueue(struct workqueue_struct *wq); 



## 5. 下半部机制的选择

可供选择的下半部机制有：软中断、 tasklet 和工作队列。 tasklet 基于软中断实现，所以两者很相近。工作队列机制与它们完全不同，它靠内核线程实现。

从设计的角度考虑，**软中断提供的执行序列化的保障最少**。这就要求软中断处理函数必须格外小心地**采取一些步骤确保共享数据的安全**，两个甚至更多相同类别的软中断有可能在不同的处理器上同时执行。如果被考察的代码本身多线索化的工作就做得非常好，比如网络子系统，它完全使用单处理器变量，那么软中断就是非常好的选择。**对于时间要求严格和执行频率很高的应用来说，它执行得也最快**。

如果代码多线索化考虑得并不充分，那么选择 tasklet 意义更大。它的**接口非常简单**，而且，由于**两个同种类型的 tasklet 不能同时执行**，所以实现起来也会简单一些。 tasklet 是有效的软中断，但**不能并发运行**。驱动程序开发者应当尽可能选择 tasklet 而不是软中断，当然，如果准备利用每一处理器上的变量或者类似的情形，以确保软中断能安全地在多个处理器上并发地运行，那么还是选择软中断。

如果你**需要把任务推后到进程上下文中完成**，那么在这三者中就只能选择工作队列了。如果进程上下文并不是必须的条件（明确点说，就是如果并不**需要睡眠**），那么软中断和 tasklet 可能更合适。**工作队列造成的开销最大，因为它要牵扯到内核线程甚至是上下文切换**。这并不是说工作队列的效率低，如果每秒钟有几千次中断，就像网络子系统时常经历的那样，那么采用其他的机制可能更合适一些。尽管如此，针对大部分情况，工作队列都能提供足够的支持。

如果讲到**易于使用**，**工作队列就当仁不让了**。使用缺省的 events 队列简直不费吹灰之力。**接下来是 tasklet**, 它的接口也很简单。**最后才是软中断**，它必须静态创建，并且需要慎重考虑其实现。

对下半部的比较：

|  下半部  | 上下文 |          顺序执行保障          |
| :------: | :----: | :----------------------------: |
|  软中断  |  中断  |              没有              |
| tasklet  |  中断  |       同类型不能同时执行       |
| 工作队列 |  进程  | 没有（和进程上下文一样被调度） |

简单地说，一般的驱动程序的编写者需要做两个选择。首先，你**是不是需要一个可调度的实体来执行需要推后完成的工作**——从根本上来说，**你有休眠的需要吗？要是有，工作队列就是你的唯一选择**。**否则最好用 tasklet** 。要是必须专注于性能的提高，那么再考虑软中断。



## 6. 在下半部之间加锁

使用下半部机制时，即使是在一个单处理器系统上，避免共享数据被同时访问也是至关重要的。**一个下半部实际上可能在任何时候执行**。

使用 tasklet 的一个好处在于，它自己负责执行的序列化保障：两个相同类型的 tasklet不允许同时执行，即使在不同的处理器上也不行。**无须为 tasklet 内的同步问题操心，但 tasklet 之间的同步（就是当两个不同类型的 tasklet 共享同一数据时）需要正确使用锁机制。**

如果**进程上下文和一个下半部共享数据**，在访问这些数据之前，你需要禁止下半部的处理并得到锁的使用权。做这些是为了本地和 SMP 的保护并且防止死锁的出现。

如果**中断上下文和一个下半部共享数据**，在访问数据之前，你需要禁止中断并得到锁的使用权。所做的这些也是为了本地和 SMP 的保护并且防止死锁的出现。

任何在工作队列中被共享的数据也需要使用锁机制。其中有关锁的要点和在一般内核代码中没什么区别，因为工作队列本来就是在进程上下文中执行的。



## 7. 禁止下半部

一般单纯禁止下半部的处理是不够的。为了保证共享数据的安全，更常见的做法是，**先得到一个锁然后再禁止下半部的处理**。

如果需要禁止所有的下半部处理（明确点说，就是所有的软中断和所有的 tasklet) ，可以调用 local_bh_ diasble() 函数。允许下半部进行处理，可以调用 local_bh_ enable() 函数。

下半部机制控制函数的清单:

|          函数           |                  描述                   |
| :---------------------: | :-------------------------------------: |
| void local_bh_disable() | 禁止本地处理器的软中断和 tasklet 的处理 |
| void local_bh_enable()  | 激活本地处理器的软中断和 tasklet 的处理 |

这些函数有可能被**嵌套使用**——最后被调用的 local_bh_ enable() 最终激活下半部。比如，第一次调用 local_bh_ disable()，则本地软中断处理被禁止；如果 local_bh_ disable() 被调用三次，则本地处理仍然被禁止；只有当第四次调用 local_bh_ enable() 时，软中断处理才被重新激活。上面的函数通过 preempt_count 为每个进程维护一个**计数器** 。当计数器变为 0 时，下半部才能够被处理。

这些函数并**不能禁止工作队列的执行**。因为工作队列是在进程上下文中运行的，不会涉及异步执行的问题，所以也就没有必要禁止它们执行。由于**软中断和 tasklet 是异步发生**的（就是说，在中断处理返回的时候），所以，内核代码必须禁止它们。另一方面，对于工作队列来说，它保护共享数据所做的工作和其他任何进程上下文中所做的都差不多。



# 第十章 内核同步方法

## 1. 原子操作

**原子操作是由编译器来保证的，保证一个线程对数据的操作不会被其他线程打断。**

原子操作可把读取和增加变量的行为包含在一个单步中执行，从而防止了竞争的发生保证了操作结果总是一致的。两个原子操作绝对不可能并发地访问同一个变量，这样加操作也就绝不可能引起竞争。

原子操作有2类：

1. 原子整数操作，有32位和64位。头文件分别为<asm/atomic.h>和<asm/atomic64.h>
2. 原子位操作。头文件 <asm/bitops.h>

原子操作的api很简单，参见相应的头文件即可。

原子操作头文件与具体的体系结构有关，比如x86架构的相关头文件在 arch/x86/include/asm/*.h

原子操作通常是**内联函数**，往往是通过**内嵌汇编指令来实现**的。如果某个函数本来就是原子的，那么它往往会被定义成一个宏。例如，在大部分体系结构上，读取一个字本身就是一种原子操作，也就是说，**在对一个字进行写入操作期间不可能完成对该字的读取**。这样，把 atomic_read(）定义成一个宏，只须返回 atomic_t 类型的整数值就可以了。



## 2. 自旋锁

自旋锁最多只能被一个可执行线程持有。如果一个执行线程试图获得一个被已经持有（即所谓的争用）的自旋锁，那么该线程就会一直进行**忙循环**—旋转—等待锁重新可用。

因为自旋锁在同一时刻至多被一个执行线程持有，所以一个时刻只能有一个线程位于临界区内，这就为多处理器机器提供了防止并发访问所需的保护机制。

一个被争用的自旋锁**使得请求它的线程在等待锁重新可用时自旋**（特别浪费处理器时间），这种行为是自旋锁的要点。

注意：**自旋锁**在发生争用时，使得等待的线程进行**忙等待**；**信号量**使得等待的线程能投入**睡眠**，而不是旋转。注意忙循环与睡眠的区别！



注意：**自旋锁是不可递归的！！！**

> Linux 内核实现的自旋锁是不可递归的，这点不同于自旋锁在其他操作系统中的实现。所以如果你试图得到一个你正持有的锁，你必须自旋，等待你自己释放这个锁。但你处于自旋忙等待中，所以你永远没有机会释放锁，于是你被自己锁死了。千万小心自旋锁！



注意：**自旋锁可以在中断处理程序中使用**，**但信号量不可在中断中使用**，因为它会导致睡眠！！！



注意：**在中断处理程序中使用自旋锁时，一定要在获取锁之前，首先禁止本地中断（在当前处理器上的中断请求）！！！**

> 否则，中断处理程序就会打断正持有锁的内核代码，有可能会试图去争用这个已被持有的自旋锁。这样一来，中断处理程序就会自旋，等待该锁重新可用，但是锁的持有者在这个中断处理程序执行完毕前不可能运行。这正是我们在前面的内容中提到的双重请求死锁。注意，需要关闭的只是当前处理器上的中断。如果中断发生在不同的处理器上，即使中断处理程序在同一锁上自旋，也不会妨碍锁的持有者（在不同处理器上）最终释放锁。



自旋锁提供了一种快速简单的锁实现方法。如果**加锁时间不长并且代码不会睡眠**（比如中断处理程序），利用自旋锁是最佳选择。如果加锁时间可能很长或者代码在持有锁时有可能睡眠，那么最好使用**信号量**来完成加锁功能。



### 2.1 自旋锁使用方法

自旋锁的基本使用形式：

```c
DEFINE_SPINLOCK(mr_lock); 
spin_lock(&mr_lock); 
/*临界区...*/
spin_unlock(&mr_lock) ; 
```



内核还提供了禁止中断同时请求锁的接口：

```c
DEFINE_SPINLOCK(mr_lock); 
unsigned long flags; 

spin_lock_irqsave(&mr_lock,flags); 
/*临界区...*/
spin_unlock_irqrestore(&mr_lock,flags);
```

> 函数 spin_lock_ irqsave() 保存中断的当前状态，并禁止本地中断，然后再去获取指定的锁。反过来 spin_unlock_ irqrestore() 对指定的锁解锁，然后让中断恢复到加锁前的状态。



如果你能确定中断在加锁前是激活的，那就不需要在解锁后恢复中断以前的状态了。你可以无条件地在解锁时激活中断。这时，使用 spin_lock_irq() 和 spin_unlock_irq() 会更好一些：

```c
DEFINE_SPINLOCK(mr_lock); 
spin_lock_irq(&mr_lock); 
/*临界区...*/
spin_unlock_irq(&mr_lock) ; 
```



自旋锁方法列表如下：

| **方法**               | **描述**                                             |
| ---------------------- | ---------------------------------------------------- |
| spin_lock()            | 获取指定的自旋锁                                     |
| spin_lock_irq()        | 禁止本地中断并获取指定的锁                           |
| spin_lock_irqsave()    | 保存本地中断的当前状态，禁止本地中断，并获取指定的锁 |
| spin_unlock()          | 释放指定的锁                                         |
| spin_unlock_irq()      | 释放指定的锁，并激活本地中断                         |
| spin_unlock_irqstore() | 释放指定的锁，并让本地中断恢复到以前状态             |
| spin_lock_init()       | 动态初始化指定的spinlock_t                           |
| spin_trylock()         | 试图获取指定的锁，如果未获取，则返回0                |
| spin_is_locked()       | 如果指定的锁当前正在被获取，则返回非0，否则返回0     |



### 2.2 自旋锁和下半部

在与下半部配合使用时，必须小心地使用锁机制。函数 **spin_lock_ bh()** 用于获取指定锁，同时它会**禁止所有下半部的执行**。相应的spin_unlock_bh() 函数执行相反的操作。

中断处理下半部的操作中使用自旋锁尤其需要小心：

1. 下半部处理和进程上下文共享数据时，由于下半部的处理可以抢占进程上下文的代码， 所以**进程上下文在对共享数据加锁前要禁止下半部的执行**，解锁时再允许下半部的执行。中断处理程序（上半部）和下半部处理共享数据时，由于中断处理（上半部）可以抢占下半部的执行， 所以**下半部在对共享数据加锁前要禁止中断处理（上半部）**，解锁时再允许中断的执行。
2. 同一类tasklet不能同时运行，所以**同类tasklet中的共享数据不需要保护**。不同类tasklet中共享数据时，就**需要在访问下半部中的数据前先获得一个普通的自旋锁**。这里不需要禁止下半部，因为同一个处理器上不会有tasklet相互抢占的情况。
3. **对于软中断**，无论是否同种类型，如果数据被软中断共享，那么它**必须得到锁的保护**。这是因为，即使是同种类型的两个软中断也可以同时运行在一个系统的多个处理器上。但是，**同一处理器上的一个软中断**绝不会抢占另一个软中断，因此，**没必要禁止下半部**。



## 3. 读-写自旋锁

当写入一块数据时，不能有其他代码并发地写数据或从中读取数据，写操作要求完全互斥。另一方面，当读取数据时，只要其他程序不对数据进行写操作就行了。只要没有写操作，多个并发的读操作都是安全的。**即写时不可读不可写，读时可读不可写**。

Linux 内核提供了专门的读－写自旋锁。这种自旋锁为读和写分别提供了不同的锁。**一个或多个读任务可以并发地持有读者锁；相反，用于写的锁最多只能被一个写任务持有，而且此时不能有并发的读操作**。



读写自旋锁除了和普通自旋锁一样有自旋特性以外，还有以下特点：

1. 读锁之间是共享的
    即一个线程持有了读锁之后，其他线程也可以以读的方式持有这个锁

2. 写锁之间是互斥的
    即一个线程持有了写锁之后，其他线程不能以读或者写的方式持有这个锁

3. 读写锁之间是互斥的
    即一个线程持有了读锁之后，其他线程不能以写的方式持有这个锁



初始化读/写自旋锁：

```c
DEFINE_RWLOCK(mr_rwlock); 
```

通常情况下，读锁和写锁会位千完全分割开的代码分支中

读者的代码分支：

```c
read_lock(&mr_rwlock); 
/* 临界区（只读）…*/
read_unlock(&mr_rwlock); 
```

在写者的代码分支:

```c
write_lock(&mr_rwlock); 
/* 临界区（只读）…*/
write_unlock(&mr_rwlock); 
```



注意，不能把一个读锁 “升级” 为写锁:

```c
read_lock(&mr_rwlock); 
write_lock(&mr_rwlock); 
```

执行上述两个函数将会带来死锁，因为写锁会不断自旋，等待所有的读者释放锁，其中也包括它自己。所以**当确实需要写操作时，要在一开始就请求写锁**。如果写和读不能清晰地分开的话，那么使用一般的自旋锁就行了，不要使用读-写自旋锁。



多个读者可以安全地获得同一个读锁，事实上，即使一个线程递归地获得同一读锁也是安全的。

如果在中断处理程序中只有读操作而没有写操作，那么，就可以混合使用 “中断禁止“ 锁，使用 read_lock() 而不是 read_lock_irqsave() 对读进行保护。不过，你还是需要用 write_lock_ irqsave() 禁止有写操作的中断，否则，中断里的读操作就有可能锁死在写锁上（假如读者正在进行操作，包含写操作的中断发生了，由于读锁还没有全部被释放，所以写操作会自旋，而读操作只能在包含写操作的中断返回后才能继续，释放读锁，此时死锁就发生）。

Linux中的读-写锁机制照顾读比照顾写要多一点。**当读锁被持有时，写操作为了互斥访问只能等待**，但是，读者却可以继续成功地作占用锁。而自旋等待的写者在所有读者释放锁之前是无法获得锁的。所以，大量读者必定会使挂起的写者处于饥饿状态。



读-写锁的相关函数如下：

| **方法**                  | **描述**                                           |
| ------------------------- | -------------------------------------------------- |
| read_lock()               | 获取指定的读锁                                     |
| read_lock_irq()           | 禁止本地中断并获得指定读锁                         |
| read_lock_irqsave()       | 存储本地中断的当前状态，禁止本地中断并获得指定读锁 |
| read_unlock()             | 释放指定的读锁                                     |
| read_unlock_irq()         | 释放指定的读锁并激活本地中断                       |
| read_unlock_irqrestore()  | 释放指定的读锁并将本地中断恢复到指定前的状态       |
| write_lock()              | 获得指定的写锁                                     |
| write_lock_irq()          | 禁止本地中断并获得指定写锁                         |
| write_lock_irqsave()      | 存储本地中断的当前状态，禁止本地中断并获得指定写锁 |
| write_unlock()            | 释放指定的写锁                                     |
| write_unlock_irq()        | 释放指定的写锁并激活本地中断                       |
| write_unlock_irqrestore() | 释放指定的写锁并将本地中断恢复到指定前的状态       |
| write_trylock()           | 试图获得指定的写锁；如果写锁不可用，返回非0值      |
| rwlock_init()             | 初始化指定的rwlock_t                               |



## 4. 信号量

信号量也是一种锁，和自旋锁不同的是，线程获取不到信号量的时候，不会像自旋锁一样循环的去试图获取锁，而是将线程**加入等待队列**，使得线程**进入睡眠**，直至有信号量释放出来时，才会唤醒睡眠的线程，进入临界区执行。

信号量使用场景：

- 由于使用信号量时，线程会睡眠，所以**等待的过程不会占用CPU时间**。所以信号量**适用于等待时间较长的临界区**。信号量消耗CPU时间的地方在于使**线程睡眠和唤醒线程**，如果 （使线程睡眠 + 唤醒线程）的CPU时间 > 线程自旋等待的CPU时间，那么可以考虑使用自旋锁。

- 信号量不同于自旋锁，它不会禁止内核抢占，所以**持有信号量的代码可以被抢占**。这意味着信号量不会对调度的等待时间带来负面影响。



### 4.1 计数信号量和二值信号量

信号量可以**同时允许任意数量的锁持有者**，而自旋锁在一个时刻最多允许一个任务持有它。

信号量有二值信号量和计数信号量 2 种，其中二值信号量比较常用：

- **二值信号量**表示信号量只有 2 个值，即 0 和 1。信号量为 1 时，表示临界区可用，信号量为 0 时，表示临界区不可访问。

  二值信号量表面看和自旋锁很相似，区别在于争用自旋锁的线程会一直**忙循环**尝试获取自旋锁，而争用信号量的线程在信号量为0时，会进入**睡眠**，信号量可用时再被唤醒。 

- **计数信号量**有个计数值，比如计数值为 5，表示同时可以有 5 个线程访问临界区。



信号量一种常用的锁机制。信号量支持两个原子操作 P() 和 V()，后来的系统把两种操作分别叫做 down() 和 up()，**降低 (down) 一个信号量就等于获取该信号量**。相反，当临界区中的操作完成后， up() 操作用来释放信号量，该操作也被称作是提升 (upping) 信号量，因为它会增加信号量的计数值。如果在该信号量上的等待队列不为空，那么处于队列中等待的任务在被唤醒的同时会获得该信号量。



### 4.2 信号量使用方法

信号量的实现是与体系结构相关的，具体实现定义在文件 <asm/semaphore.h> 中， struct semaphore 类型用来表示信号量。

使用信号量的方法如下：

1. 创建和初始化信号量：

    ```c
    /* 静态声明信号量，name是信号量变量名， count是信号扯的使用数量 */
    struct  semaphore name; 
    sema_init(&name, count);

    /* 互斥信号量的创建可以使用以下快捷方式 */
    static DECLARE_MUTEX(name);

    /* 信号量作为一个大数据结构的一部分动态创建，sem是指针 */
    sema_init(sem, count);

    /* 初始化一个动态创建的互斥信号量 */
    init_MUTEX(sem) ; 
    ```

2. 使用信号量

   函数 down_interruptible() 试图获取指定的信号量，如果信号量不可用，它将把调用进程置成 TASK_ INTERRUPTIBLE 状态——进入睡眠。如果进程在等待获取信号量的时候接收到了信号，那么该进程就会被唤醒，而函数 down_interruptible() 会返回-EINTR 。
   
   > 另外一个函数 down() 会让进程在 TASK_UNINTERRUPTIBLE 状态下睡眠。你应该不希望这种情况发生，因为这样一来，进程在等待信号量的时候就不再响应信号了。因此，使用 down_interruptible(）比使用 down() 更为普遍（也更正确）
   >
   > 对于 TASK_INTERRUPTIBLE 和 TASK_UNINTERRUPTIBLE 补充说明一下：
   >
   > - TASK_INTERRUPTIBLE - 可打断睡眠，可以接受信号并被唤醒，也可以在等待条件全部达成后被显式唤醒(比如wake_up()函数)。
   > - TASK_UNINTERRUPTIBLE - 不可打断睡眠，只能在等待条件全部达成后被显式唤醒(比如wake_up()函数)。
   
   ```c
   /* 试图获取信号量....， 信号未获取成功时，进入睡眠
    * 此时，线程状态为 TASK_INTERRUPTIBLE
    */
   down_interruptible(&mr_sem);
   /* 这里也可以用：
    * down(&mr_sem);
   ```
 * 这个方法把线程状态置为 TASK_UNINTERRUPTIBLE 后睡眠
    */
   
   /* 临界区 ... */
   
   /* 释放给定的信号量 */
   up(&mr_sem);
   ```
   
   使用 down_trylock() 函数，可以以堵塞方式来获取指定的信号量。在信号量已被占用时，它立刻返回非 0 值；否则，它返回 0, 而且让你成功持有信号量锁。
   ```



信号量方法总结如下：

| **方法**                               | **描述**                                                     |
| -------------------------------------- | ------------------------------------------------------------ |
| sema_init(struct semaphore *, int)     | 以指定的计数值初始化动态创建的信号量                         |
| init_MUTEX(struct semaphore *)         | 以计数值1初始化动态创建的信号量                              |
| init_MUTEX_LOCKED(struct semaphore *)  | 以计数值0初始化动态创建的信号量（初始为加锁状态）            |
| down_interruptible(struct semaphore *) | 以试图获得指定的信号量，如果信号量已被争用，则进入可中断睡眠状态 |
| down(struct semaphore *)               | 以试图获得指定的信号量，如果信号量已被争用，则进入不可中断睡眠状态 |
| down_trylock(struct semaphore *)       | 以试图获得指定的信号量，如果信号量已被争用，则立即返回非0值  |
| up(struct semaphore *)                 | 以释放指定的信号量，如果睡眠队列不空，则唤醒其中一个任务     |



信号量结构体具体如下：

```c
/* Please don't access any members of this structure directly */
struct semaphore {
    spinlock_t        lock;
    unsigned int        count;
    struct list_head    wait_list;
};
```

可以发现**信号量结构体中有个自旋锁**，这个自旋锁的**作用是保证信号量的down和up等操作不会被中断处理程序打断**。



## 5. 读写信号量

读写信号量和信号量之间的关系与读写自旋锁和普通自旋锁之间的关系差不多。**读写信号量都是二值信号量**，即计数值最大为1。**只对写者互斥，不对读者**。增加读者时，计数器不变，增加写者，计数器才减一。也就是说读写信号量保护的临界区，最多只有一个写者，但可以有多个读者。只要没有写者，并发持有读锁的读者数不限。

读-写信号量在内核中是由 rw_ semaphore 结构表示的，定义在文件 <linwc/rwsem.h> 中。

信号量的创建：

```c
/* 创建静态声明的读-写信号量 */
static DECLARE_RWSEM(name);

/* 创建动态声明的读-写信号量 */
init_rwsem(struct rw_sernaphore *sem);
```

所有读-写锁的睡眠都不会被信号打断，所以它只有一个版本的down() 操作：

```c
static DECLARE_RWSEM(mr_rwsem);

/* 试图获取信号量用于读...*/
down_read(&mr_rwsem);

/* 临界区（只读）... */

/* 释放信号量 */
up_read(&mr_rwsem);
/* ... */

/* 试图获取信号最用于写...*/
down_write(&mr_rwsem);

/* 临界区（读和写）．．．*/

/* 释放信号量 */
up_write(&mr_sem);
```

与标准信号量一样，读-写信号量也提供了 down_read_ trylock() 和 down_write_trylock() 方法。这两个方法都需要一个指向读-写信号量的指针作为参数。如果成功获得了信号量锁，它们返回非0值；如果信号量锁被争用，则返回0。注意：这与普通信号量的情形完全相反。

读-写信号量相比读-写自旋锁多一种特有的操作：downgrade_write()。这个函数可以动态地将获取的写锁转换为读锁。

使用原则：**只有当代码中的读和写可以明白无误地分割开来时，才使用读-写信号量**。



## 6. 互斥体

互斥体也是一种**可以睡眠**的锁，相当于二值信号量，只是提供的API更加简单，使用的场景也更严格一些，如下所示：

1. mutex的计数值只能为1，**任何时刻中只有一个任务可以持有 mutex**，也就是最多只允许一个线程访问临界区
2. **必须在同一上下文中上锁和解锁**，因而 mutex 不适合内核同用户空间复杂的同步场景
3. **不能递归的上锁和解锁**，即不能递归地持有同一个锁，也不能再解锁一个已经被解开的 mutex
4. 持有一个 mutex 时，进程不能退出
5. **「mutex 不能在中断或者下半部中使用」**，也就是mutex只能在进程上下文中使用
6. mutex 只能通过官方 API 来管理，只能使用 API 进行初始化，不可被拷贝、手动初始化或者重复初始化



### 1. mutex 使用方法

静态地定义 mutex：

```c
DEFINE_MUTEX(name);
```

动态初始化 mutex：

```c
mutex_init(&mutex);
```

对互斥锁锁定和解锁：

```c
mutex_lock(&mutex); 
/* 临界区 */
mutex_unlock(&mutex);
```

mutex 操作列表：

| **方法**                        | **描述**                                                   |
| ------------------------------- | ---------------------------------------------------------- |
| mutex_lock(struct mutex *)      | 为指定的 mutex 上锁，如果锁不可用则睡眠                    |
| mutex_unlock(struct mutex *)    | 为指定的 mutex 解锁                                        |
| mutex_trylock(struct mutex *)   | 试图获取指定的 mutex，如果成功则返回1；否则锁被获取，返回0 |
| mutex_is_locked(struct mutex *) | 如果锁已被争用，则返回1；否则返回0                         |



### 2. 信号量和互斥体

互斥体和信号量很相似，内核中两者共存会令人混淆。除非 mutex 的某个约束妨碍你使用，否则优先使用 mutex 。当你写新代码时，只有碰到特殊场合（一般是很底层代码）才会需要使用信号量。因此建议**首选 mutex**。如果发现不能满足其约束条件，且没有其他别的选择时，再考虑选择信号量。



### 3. 自旋锁和互斥体

**「★在中断上下文中只能使用自旋锁，而在任务睡眠时只能使用互斥体★」**

自旋锁和信号量的比较：

| **需求**             | **建议的加锁方法** |
| -------------------- | ------------------ |
| 低开销加锁           | 优先使用自旋锁     |
| 短期锁定             | 优先使用自旋锁     |
| 长期加锁             | 优先使用互斥体     |
| **中断上下文中加锁** | **使用自旋锁**     |
| **持有锁需要睡眠**   | **使用互斥体**     |



## 7. 完成变量

如果在内核中一个任务需要发出信号通知另一任务发生了某个特定事件，利用完成变量 (completion variable) 是使**两个任务得以同步的简单方法**。完成变量的机制类似于信号量，比如一个线程 A 进入临界区之后，另一个线程 B 会在完成变量上等待，线程 A 完成了任务出了临界区之后，使用完成变量来唤醒线程 B。

完成变量仅仅提供了代替信号量的一个简单的解决方法。例如，当子进程执行或者退出时， vfork() 系统调用使用完成变量唤醒父进程。

静态地创建并初始化完成变量：

```c
DECLARE_COMPLETION(mr_comp); 
```

动态创建并初始化完成变量：

```c
init_completion(struct completion *);
```

在一个指定的完成变量上，需要等待的任务调用 wait_for_ completion() 来等待特定事件。当特定事件发生后，产生事件的任务调用 complete() 来发送信号唤醒正在等待的任务。

完成变量 API：

| **方法**                                 | **描述**                       |
| ---------------------------------------- | ------------------------------ |
| init_completion(struct completion *)     | 初始化指定的动态创建的完成变量 |
| wait_for_completion(struct completion *) | 等待指定的完成变量接受信号     |
| complete(struct completion *)            | 发信号唤醒任何等待任务         |

完成变量的通常用法是，将完成变量作为数据结构中的一项动态创建，而完成数据结构初始化工作的内核代码将调用wait_for_ 

completion() 进行等待。初始化完成后，初始化函数调用 completion() 唤醒在等待的内核任务。



## 8. 大内核锁-BLK

BKL（大内核锁）是一个**全局自旋锁**，使用它主要是为了方便实现从 Linux 最初的 SMP 过渡到细粒度加锁机制。

特性：

- 持有 BKL 的任务仍然可以睡眠。因为当任务无法被调度时，所加锁会自动被丢弃；当任务被调度时，锁又会被重新获得。当然，这并不是说，当任务持有 BKL 时，睡眠是安全的，仅仅是可以这样做，因为睡眠不会造成任务死锁。
- BKL 是一种递归锁。一个进程可以多次请求一个锁，并不会像自旋锁那样产生死锁现象。
- BKL 只可以用在进程上下文中。和自旋锁不同，你不能在中断上下文中申请 BLK。
- 新的用户不允许使用 BLK 。随着内核版本的不断前进，越来越少的驱动和子系统再依赖于 BLK 了。

BKL 在被持有时同样会禁止内核抢占。在单一处理器内核中， BKL 并不执行实际的加锁操作。



## 9. 顺序锁

顺序锁为**读写共享数据**提供了一种**简单**的实现机制。

- 读写自旋锁和读写信号量，在读锁被获取之后，写锁是不能再被获取的，也就是说，必须等所有的读锁释放后，才能对临界区进行写入操作。

- 顺序锁则不同，**读锁被获取的情况下，写锁仍然可以被获取**。使用顺序锁的读操作在读之前和读之后都会检查顺序锁的序列值，如果前后值不符，则说明在读的过程中有写操作发生，那么读操作会重新执行一次，直至读前后的序列值是一样的。

定义一个 seq 锁：

```c
seqlock_t mr_seq_ock = DEFINE_SEQLOCK(mr_seq_ock); 
```

写锁的方法：

```c
write_seqlock(&mr_seq_lock); 
/* 写锁被获取... */
write—sequnlock(&mr_seq_lock);
```

上面和普通自旋锁类似。不同的情况发生在读时，并且与自旋锁有很大不同：

```c
unsigned long seq; 
do {
    /* 读之前获取顺序锁 mr_seq_lock 的序列值 */
    seq = read_seqbegin(&mr_seq_lock);
	... // 读数据
} while(read_seqretry(&mr_seq_lock, seq)); /* 顺序锁foo此时的序列值!=seq 时返回true，反之返回false */
```

顺序锁优先保证写锁的可用，所以**适用于那些读者很多，写者很少，写优于读，数据简单的场景**。



## 10. 禁止抢占

其实使用自旋锁已经可以防止内核抢占了，但是有时候**仅仅需要禁止内核抢占，不需要像自旋锁那样连中断都屏蔽掉**。

这时候就需要使用禁止内核抢占的方法了：

| **方法**                    | **描述**                                                    |
| --------------------------- | ----------------------------------------------------------- |
| preempt_disable()           | 增加抢占计数值，从而禁止内核抢占                            |
| preempt_enable()            | 减少抢占计算，并当该值降为0时检查和执行被挂起的需调度的任务 |
| preempt_enable_no_resched() | 激活内核抢占但不再检查任何被挂起的需调度的任务              |
| preempt_count()             | 返回抢占计数                                                |

这里的preempt_disable()和preempt_enable()是可以嵌套调用的，disable和enable的次数最终应该是一样的。



## 11. 顺序和屏障

对于一段代码，编译器或者处理器在编译和执行时，为了提高效率，可能会对读和写重新排序，从而使得代码的执行顺序和我们写的代码有些区别。

所有可能重新排序和写的处理器提供了机器指令来确保顺序要求。同样也可以指示编译器不要对给定点周围的指令序列进行重新排序。这些确保顺序的指令称作屏障 (barriers) 。

一般情况下，这没有什么问题，但是在并发条件下，可能会出现取得的值与预期不一致的情况：

```c
/* 
 * 线程A和线程B共享的变量 a和b
 * 初始值 a=1, b=2
 */
int a = 1, b = 2;

/*
 * 假设线程A 中对 a和b的操作
 */
void Thread_A() {
    a = 5;
    b = 4;
}

/*
 * 假设线程B 中对 a和b的操作
 */
void Thread_B() {
    if (b == 4)
        printf("a = %d\n", a);
}
```

由于编译器或者处理器的优化，线程A中的赋值顺序可能是b先赋值后，a才被赋值。

所以如果线程A中 b=4; 执行完，a=5; 还没有执行的时候，线程B开始执行，那么线程B打印的是a的初始值1。

这就与我们预期的不一致了，我们预期的是a在b之前赋值，所以线程B要么不打印内容，如果打印的话，a的值应该是5。



**在某些并发情况下，为了保证代码的执行顺序，引入了一系列屏障方法来阻止编译器和处理器的优化。**

| **方法**                   | **描述**                                                     |
| -------------------------- | ------------------------------------------------------------ |
| rmb()                      | 阻止跨越屏障的载入动作发生重排序                             |
| read_barrier_depends()     | 阻止跨越屏障的具有数据依赖关系的载入动作重排序               |
| wmb()                      | 阻止跨越屏障的存储动作发生重排序                             |
| mb()                       | 阻止跨越屏障的载入和存储动作重新排序                         |
| smp_rmb()                  | 在SMP上提供rmb()功能，在UP上提供barrier()功能                |
| smp_read_barrier_depends() | 在SMP上提供read_barrier_depends()功能，在UP上提供barrier()功能 |
| smp_wmb()                  | 在SMP上提供wmb()功能，在UP上提供barrier()功能                |
| smp_mb()                   | 在SMP上提供mb()功能，在UP上提供barrier()功能                 |
| barrier()                  | 阻止编译器跨越屏障对载入或存储操作进行优化                   |

- rmb() 方法提供了一个“读“内存屏障，它确保跨越 rmb() 的载入动作不会发生重排序。也就是说，在 rmb(）之前的载入操作不会被重新排在该调用之后，同理，在 rmb() 之后的载入操作不会被重新排在该调用之前。
- wmb() 方法提供了一个“写“内存屏障，这个函数的功能和 rmb() 类似，区别仅仅是它是针对存储而非载入一它确保跨越屏障的存储不发生重排序。
- mb() 方法既提供了读屏障也提供了写屏障。
- read_barrier_depends() 是 rmb() 的变种，它提供了一个读屏障，但是仅仅是针对后续读操作所依靠的那些载入。
- barrier() 方法可以防止编译器跨屏障对载入或存储操作进行优化。编译器不会重新组织存储或载入操作，而防止改变 C 代码的效果和现有数据的依赖关系。但是，它不知道在当前上下文之外会发生什么事。例如，编译器不可能知道有中断发生，这个中断有可能在读取正在被写入的数据。这时就要求存储操作发生在读取操作前。内存屏障可以完成编译器屏障的功能，但是编译器屏障要比内存屏障轻最（它实际上是轻快的）得多。

为了使得上面的小例子能正确执行，用上表中的函数修改线程A的函数即可：

```c
/*
 * 假设线程A 中对 a和b的操作
 */
void Thread_A()
{
    a = 5;
    mb(); 
    /* 
     * mb()保证在对b进行载入和存储值(值就是4)的操作之前
     * mb()代码之前的所有载入和存储值的操作全部完成(即 a = 5;已经完成)
     * 只要保证a的赋值在b的赋值之前进行，那么线程B的执行结果就和预期一样了
     */
    b = 4;
}
```



## 12. 总结

本节讨论了大约11种内核同步方法，除了大内核锁已经不再推荐使用之外，其他各种锁都有其适用的场景。

了解了各种同步方法的适用场景，才能正确的使用它们，使我们的代码在安全的保障下达到最优的性能。

同步的目的就是为了**保障数据的安全**，其实就是保障各个线程之间共享资源的安全，下面根据共享资源的情况来讨论一下10种同步方法的选择。10种同步方法在图中分别用蓝色框标出。

<img src="https://gitee.com//MrRen-sdhm/Images/raw/master/img/20200702224251.png" alt="01111600-82b1e0017e7746d084cd3e8da2f26b4d" style="zoom:80%;" />